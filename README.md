# ğŸŒ¤ï¸ Weather Data Engineering Pipeline

[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![Python 3.8+](https://img.shields.io/badge/python-3.8+-blue.svg)](https://www.python.org/downloads/)
[![Docker](https://img.shields.io/badge/docker-%230db7ed.svg?style=flat&logo=docker&logoColor=white)](https://www.docker.com/)

A **production-ready, end-to-end weather data engineering pipeline** that automates extraction, transformation, modeling, and visualization. Built using **Airflow**, **DBT**, **PostgreSQL**, **Docker**, and **Superset**.  

---

## ğŸ“Š Live Dashboard Preview

![Dashboard Overview](docs/dashboard/dashboard_overview.jpg)

### Key Metrics & Visualizations
- ğŸŒ¡ï¸ Temperature & humidity trends (hourly/daily)
- ğŸŒ¬ï¸ Wind speed & atmospheric pressure analysis
- â˜ï¸ Cloud cover & precipitation monitoring
- ğŸ“ˆ Correlation heatmaps
- ğŸ“Š Dynamic KPI gauges & time-series charts

---

## ğŸ—ï¸ Architecture Overview

![Architecture Diagram](docs/images/architecture_overview.png)

**Data Flow**:

1. **Extract**: Fetch real-time weather data from [Open-Meteo API](https://open-meteo.com/en/docs)  
2. **Transform**: Clean & standardize using Pandas  
3. **Load**: Persist to PostgreSQL database  
4. **Model**: DBT transformations (staging â†’ marts)  
5. **Orchestrate**: Schedule & monitor via Airflow DAGs  
6. **Visualize**: Interactive dashboards in Superset  

---

## ğŸ› ï¸ Technology Stack

| Component | Technology | Purpose |
|-----------|-----------|---------|
| Orchestration | Apache Airflow 2.x | Workflow automation & scheduling |
| Transformation | DBT Core | SQL-based modeling & testing |
| Database | PostgreSQL 14+ | Data warehouse |
| Visualization | Apache Superset 3.x | BI dashboards |
| Containerization | Docker & Compose | Environment consistency |
| API | Open-Meteo API | Weather data source |
| Language | Python 3.8+ | ELT scripts |

---

## ğŸ“ Project Structure

```
.
â”œâ”€â”€ README.md
â”œâ”€â”€ __pycache__
â”‚   â””â”€â”€ api_request.cpython-312.pyc
â”œâ”€â”€ airflow
â”‚   â””â”€â”€ dags
â”œâ”€â”€ api-request
â”‚   â”œâ”€â”€ __pycache__
â”‚   â”œâ”€â”€ api_request.py
â”‚   â”œâ”€â”€ insert_records.py
â”‚   â””â”€â”€ transform_data.py
â”œâ”€â”€ dbt
â”‚   â”œâ”€â”€ dbt_internal_packages
â”‚   â”œâ”€â”€ dbt_project.yml
â”‚   â”œâ”€â”€ logs
â”‚   â”œâ”€â”€ my_project
â”‚   â”œâ”€â”€ profiles.yml
â”‚   â””â”€â”€ target
â”œâ”€â”€ docker
â”‚   â”œâ”€â”€ docker-bootstrap.sh
â”‚   â”œâ”€â”€ docker-init.sh
â”‚   â””â”€â”€ superset_config.py
â”œâ”€â”€ docker-compose.yml
â”œâ”€â”€ docs
â”‚   â”œâ”€â”€ dashboard
â”‚   â””â”€â”€ images
â”œâ”€â”€ postgres
â”‚   â”œâ”€â”€ airflow_init.sql
â”‚   â””â”€â”€ superset_init.sql
â””â”€â”€ venv
    â”œâ”€â”€ bin
    â”œâ”€â”€ lib
    â”œâ”€â”€ lib64
    â””â”€â”€ pyvenv.cfg
```
---

## âš ï¸ LF / CRLF Line Endings Issue (Windows)

Previously, working on Windows caused issues with Git and Docker due to inconsistent line endings (CRLF vs LF).  

âœ… **Solution Implemented:**
- Created a temporary branch `bugs_LF_CRLF` to fix all line endings.
- Normalized all files to LF for cross-platform compatibility.
- Merged the fix back into `main`.
- Configured Git to handle line endings automatically:
  ```bash
  git config --global core.autocrlf true  # For Windows users
---

## ğŸš€ Getting Started

### Prerequisites
- Docker Desktop v20.10+  
- Docker Compose v2.0+  
- 8GB RAM minimum  
- Available ports: 8080 (Airflow), 8088 (Superset), 5432 (PostgreSQL)

### Installation

#### 1ï¸âƒ£ Clone the Repository
```bash
git clone https://github.com/noran-salm/weather-data-project.git
cd weather-data-project

## ğŸš€ Getting Started

### Prerequisites
- Docker Desktop (v20.10+)
- Docker Compose (v2.0+)
- 8GB RAM minimum
- Ports available: 8080 (Airflow), 8088 (Superset), 5432 (PostgreSQL)

### Installation

#### 1ï¸âƒ£ Clone the Repository
```bash
git clone https://github.com/noran-salm/weather-data-project.git
cd weather-data-project
```

#### 2ï¸âƒ£ Start All Services
```bash
docker-compose up --build -d
```

**Services Starting:**
- PostgreSQL 
- Apache Airflow Webserver 
- Apache Airflow Scheduler
- DBT Container
- Apache Superset 

â±ï¸ **Wait 2-3 minutes** for services to initialize completely.

#### 3ï¸âƒ£ Verify Service Health
```bash
docker-compose ps
```

All services should show `healthy` or `running` status.

---

## ğŸ” Access Credentials

| Service | URL | Username | Password |
|---------|-----|----------|----------|
| **Airflow** | http://localhost:8080 | `admin` | Check terminal logs |
| **Superset** | http://localhost:8088 | `admin` | `123456` |

---

## âš™ï¸ Configuration & Setup

### Initialize Superset (First-Time Only)
```bash
# Create admin user
docker exec -it superset_container superset fab create-admin \
  --username admin \
  --firstname Admin \
  --lastname User \
  --email admin@superset.com \
  --password 123456

# Upgrade database
docker exec -it superset_container superset db upgrade

# Initialize Superset
docker exec -it superset_container superset init
```

### Configure Database Connections

**Superset Connection String:**
```
postgresql://airflow:airflow@postgres:5432/airflow
```

**DBT Profile (`dbt/profiles.yml`):**
```yaml
my_project:
  target: dev
  outputs:
    dev:
      type: postgres
      host: postgres
      port: 5432
      user: db_user  
      password: db_password
      dbname: db
      schema: dev
      threads: 4
```

---

## ğŸ¯ Running the Pipeline

### Option A: Airflow UI (Recommended)
1. Navigate to http://localhost:8080
2. Login with credentials
3. Locate `weather-api-dbt-orchestrator` DAG
4. Toggle **ON** to enable
5. Click **â–¶ Trigger DAG**

### Option B: Manual Execution
```bash
# Run ETL script
docker exec -it airflow_container python /opt/airflow/api-request/insert_records.py

# Run DBT models
docker exec -it dbt_container dbt run --project-dir /dbt/my_project

# Run DBT tests
docker exec -it dbt_container dbt test --project-dir /dbt/my_project
```

### Option C: Scheduled Execution
The DAG runs automatically based on the schedule defined in `orchetstrator.py`:
```python
schedule=timedelta(minutes=5)  # Adjust as needed
```

---

## ğŸ“Š Using the Dashboard

1. Open http://localhost:8088
2. Navigate to **Dashboards** â†’ **Weather Report**
3. Use filters to:
   - Select specific cities
   - Adjust date ranges
   - Compare metrics across regions
4. Export charts as PNG/PDF for reports

---

## ğŸ§ª Data Quality & Testing

### Run DBT Tests
```bash
docker exec -it dbt_container dbt test --project-dir /dbt/my_project
```

### Available Tests
- **Schema validation**: Column types and constraints
- **Null checks**: Required fields completeness
- **Uniqueness**: Primary key integrity
- **Referential integrity**: Foreign key relationships
- **Range validation**: Temperature/humidity bounds

---

## ğŸ”” Monitoring & Alerts

### View Logs
```bash
# Airflow logs
docker exec -it airflow_container cat /opt/airflow/logs/scheduler/latest/*.log

# DBT logs
docker exec -it dbt_container cat /dbt/logs/dbt.log

# Application logs
tail -f logs/pipeline.log
```

---

## ğŸ› Troubleshooting

### Common Issues

**Port Already in Use**
```bash
# Find and kill process using port 8080
lsof -ti:8080 | xargs kill -9
```

**Database Connection Failed**
```bash
# Restart PostgreSQL container
docker-compose restart postgres

# Check database health
docker exec -it postgres psql -U airflow -d airflow -c "SELECT 1;"
```

**DBT Models Not Running**
```bash
# Debug DBT connection
docker exec -it dbt_container dbt debug --project-dir /dbt/my_project
```

**Airflow DAG Not Appearing**
```bash
# Refresh DAGs
docker exec -it airflow_container airflow dags list

# Check for syntax errors
docker exec -it airflow_container python -m py_compile /opt/airflow/dags/*.py
```

---

## ğŸ§¹ Cleanup

### Stop All Services
```bash
docker-compose down
```

### Remove Volumes (Full Reset)
```bash
docker-compose down -v
```

### Remove Images
```bash
docker-compose down --rmi all
```
---

## ğŸ“š Additional Resources

- ğŸ“– [Open-Meteo API Documentation](https://open-meteo.com/en/docs)
- ğŸ› ï¸ [Apache Airflow Documentation](https://airflow.apache.org/docs/)
- ğŸ”§ [DBT Documentation](https://docs.getdbt.com/)
- ğŸ“Š [Apache Superset Documentation](https://superset.apache.org/docs/intro)
- ğŸ³ [Docker Compose Documentation](https://docs.docker.com/compose/)

---

## ğŸ¤ Contributing

Contributions are welcome! Please follow these steps:

1. Fork the repository
2. Create a feature branch (`git checkout -b feature/amazing-feature`)
3. Commit changes (`git commit -m 'Add amazing feature'`)
4. Push to branch (`git push origin feature/amazing-feature`)
5. Open a Pull Request

---

## ğŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

---

## ğŸ‘¤ About the Author

**Noran Salm** â€“ Data & Software Engineering Enthusiast  

- GitHub: [@noran-salm](https://github.com/noran-salm)  
- LinkedIn: [Connect with me](https://www.linkedin.com/in/noran-salm)  
- Passionate about building scalable data pipelines, automation, and modern analytics solutions.  
- Actively contributing to open-source projects and exploring cloud/data engineering tools like Airflow, DBT, PostgreSQL, and Azure.  

â­ If this project helped you, feel free to give it a star on GitHub!


---
