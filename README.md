# ğŸŒ¤ï¸ Weather Data Engineering Pipeline

[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![Python 3.8+](https://img.shields.io/badge/python-3.8+-blue.svg)](https://www.python.org/downloads/)
[![Docker](https://img.shields.io/badge/docker-%230db7ed.svg?style=flat&logo=docker&logoColor=white)](https://www.docker.com/)

A production-ready, end-to-end data engineering pipeline that automates weather data collection, transformation, modeling, and visualization. Built with modern data stack: **Apache Airflow**, **DBT**, **PostgreSQL**, **Docker**, and **Apache Superset**.

---

## ğŸ“Š Live Dashboard Preview

![Dashboard Overview](docs/dashboard/dashboard_overview.jpg)

### Key Metrics & Visualizations
- ğŸŒ¡ï¸ Real-time temperature and humidity trends
- ğŸŒ¬ï¸ Wind speed and atmospheric pressure analysis
- â˜ï¸ Cloud cover and precipitation monitoring
- ğŸ“ˆ Multi-dimensional correlation heatmaps
- ğŸ“Š Dynamic KPI gauges and time-series charts

---

## ğŸ—ï¸ Architecture Overview

![Architecture Diagram](docs/images/weather-data.png)

### Data Flow
1. **Extract**: Fetch real-time weather data from Open-Meteo API
2. **Transform**: Clean and standardize data using Pandas
3. **Load**: Persist to PostgreSQL database
4. **Model**: Transform with DBT (staging â†’ marts)
5. **Orchestrate**: Schedule and monitor via Apache Airflow
6. **Visualize**: Interactive dashboards in Apache Superset

---

## ğŸ› ï¸ Technology Stack

| Component | Technology | Purpose |
|-----------|-----------|---------|
| **Orchestration** | Apache Airflow 2.x | Workflow automation and scheduling |
| **Data Transformation** | DBT Core | SQL-based modeling and testing |
| **Database** | PostgreSQL 14+ | Data warehouse |
| **Visualization** | Apache Superset | BI dashboards and analytics |
| **Containerization** | Docker & Docker Compose | Environment consistency |
| **API** | Open-Meteo API | Weather data source |
| **Language** | Python 3.8+ | ELT scripts |

---

## ğŸ“ Project Structure

```
.
â”œâ”€â”€ README.md
â”œâ”€â”€ airflow/
â”‚ â””â”€â”€ dags/
â”œâ”€â”€ api-request/
â”‚ â”œâ”€â”€ api_request.py
â”‚ â”œâ”€â”€ insert_records.py
â”‚ â””â”€â”€ transform_data.py
â”œâ”€â”€ dbt/
â”‚ â”œâ”€â”€ my_project/
â”‚ â”œâ”€â”€ logs/
â”‚ â””â”€â”€ profiles.yml
â”œâ”€â”€ docker/
â”‚ â”œâ”€â”€ docker-bootstrap.sh
â”‚ â”œâ”€â”€ docker-init.sh
â”‚ â””â”€â”€ superset_config.py
â”œâ”€â”€ docker-compose.yml
â”œâ”€â”€ docs/
â”‚ â”œâ”€â”€ dashboard/
â”‚ â””â”€â”€ images/
â”‚ â””â”€â”€ weather-data.png
â”œâ”€â”€ logs/
â”‚ â””â”€â”€ dbt.log
â”œâ”€â”€ postgres/
â”‚ â”œâ”€â”€ airflow_init.sql
â”‚ â””â”€â”€ superset_init.sql
â””â”€â”€ venv/
â”œâ”€â”€ bin/
â”œâ”€â”€ lib/
â””â”€â”€ pyvenv.cfg
```

---

## ğŸš€ Getting Started

### Prerequisites
- Docker Desktop (v20.10+)
- Docker Compose (v2.0+)
- 8GB RAM minimum
- Ports available: 8080 (Airflow), 8088 (Superset), 5432 (PostgreSQL)

### Installation

#### 1ï¸âƒ£ Clone the Repository
```bash
git clone https://github.com/noran-salm/weather-data-project.git
cd weather-data-project
```

#### 2ï¸âƒ£ Start All Services
```bash
docker-compose up --build -d
```

**Services Starting:**
- PostgreSQL 
- Apache Airflow Webserver 
- Apache Airflow Scheduler
- DBT Container
- Apache Superset 

â±ï¸ **Wait 2-3 minutes** for services to initialize completely.

#### 3ï¸âƒ£ Verify Service Health
```bash
docker-compose ps
```

All services should show `healthy` or `running` status.

---

## ğŸ” Access Credentials

| Service | URL | Username | Password |
|---------|-----|----------|----------|
| **Airflow** | http://localhost:8080 | `admin` | Check terminal logs |
| **Superset** | http://localhost:8088 | `admin` | `123456` |

---

## âš™ï¸ Configuration & Setup

### Initialize Superset (First-Time Only)
```bash
# Create admin user
docker exec -it superset_container superset fab create-admin \
  --username admin \
  --firstname Admin \
  --lastname User \
  --email admin@superset.com \
  --password 123456

# Upgrade database
docker exec -it superset_container superset db upgrade

# Initialize Superset
docker exec -it superset_container superset init
```

### Configure Database Connections

**Superset Connection String:**
```
postgresql://airflow:airflow@postgres:5432/airflow
```

**DBT Profile (`dbt/profiles.yml`):**
```yaml
my_project:
  target: dev
  outputs:
    dev:
      type: postgres
      host: postgres
      port: 5432
      user: db_user  
      password: db_password
      dbname: db
      schema: dev
      threads: 4
```

---

## ğŸ¯ Running the Pipeline

### Option A: Airflow UI (Recommended)
1. Navigate to http://localhost:8080
2. Login with credentials
3. Locate `weather-api-dbt-orchestrator` DAG
4. Toggle **ON** to enable
5. Click **â–¶ Trigger DAG**

### Option B: Manual Execution
```bash
# Run ETL script
docker exec -it airflow_container python /opt/airflow/api-request/insert_records.py

# Run DBT models
docker exec -it dbt_container dbt run --project-dir /dbt/my_project

# Run DBT tests
docker exec -it dbt_container dbt test --project-dir /dbt/my_project
```

### Option C: Scheduled Execution
The DAG runs automatically based on the schedule defined in `orchetstrator.py`:
```python
schedule=timedelta(minutes=5)  # Adjust as needed
```

---

## ğŸ“Š Using the Dashboard

1. Open http://localhost:8088
2. Navigate to **Dashboards** â†’ **Weather Report**
3. Use filters to:
   - Select specific cities
   - Adjust date ranges
   - Compare metrics across regions
4. Export charts as PNG/PDF for reports

---

## ğŸ§ª Data Quality & Testing

### Run DBT Tests
```bash
docker exec -it dbt_container dbt test --project-dir /dbt/my_project
```

### Available Tests
- **Schema validation**: Column types and constraints
- **Null checks**: Required fields completeness
- **Uniqueness**: Primary key integrity
- **Referential integrity**: Foreign key relationships
- **Range validation**: Temperature/humidity bounds

---

## ğŸ”” Monitoring & Alerts

### View Logs
```bash
# Airflow logs
docker exec -it airflow_container cat /opt/airflow/logs/scheduler/latest/*.log

# DBT logs
docker exec -it dbt_container cat /dbt/logs/dbt.log

# Application logs
tail -f logs/pipeline.log
```

---

## ğŸ› Troubleshooting

### Common Issues

**Port Already in Use**
```bash
# Find and kill process using port 8080
lsof -ti:8080 | xargs kill -9
```

**Database Connection Failed**
```bash
# Restart PostgreSQL container
docker-compose restart postgres

# Check database health
docker exec -it postgres psql -U airflow -d airflow -c "SELECT 1;"
```

**DBT Models Not Running**
```bash
# Debug DBT connection
docker exec -it dbt_container dbt debug --project-dir /dbt/my_project
```

**Airflow DAG Not Appearing**
```bash
# Refresh DAGs
docker exec -it airflow_container airflow dags list

# Check for syntax errors
docker exec -it airflow_container python -m py_compile /opt/airflow/dags/*.py
```

---

## ğŸ§¹ Cleanup

### Stop All Services
```bash
docker-compose down
```

### Remove Volumes (Full Reset)
```bash
docker-compose down -v
```

### Remove Images
```bash
docker-compose down --rmi all
```
---

## ğŸ“š Additional Resources

- ğŸ“– [Open-Meteo API Documentation](https://open-meteo.com/en/docs)
- ğŸ› ï¸ [Apache Airflow Documentation](https://airflow.apache.org/docs/)
- ğŸ”§ [DBT Documentation](https://docs.getdbt.com/)
- ğŸ“Š [Apache Superset Documentation](https://superset.apache.org/docs/intro)
- ğŸ³ [Docker Compose Documentation](https://docs.docker.com/compose/)

---

## ğŸ¤ Contributing

Contributions are welcome! Please follow these steps:

1. Fork the repository
2. Create a feature branch (`git checkout -b feature/amazing-feature`)
3. Commit changes (`git commit -m 'Add amazing feature'`)
4. Push to branch (`git push origin feature/amazing-feature`)
5. Open a Pull Request

---

## ğŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

---

## ğŸ‘¤ Author

**Noran Salm**

- GitHub: [@noran-salm](https://github.com/noran-salm)
- LinkedIn: [Connect with me](https://www.linkedin.com/in/noran-salm)

---

## â­ Show Your Support

If this project helped you, please give it a â­ï¸!

---

