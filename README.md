# ğŸŒ¤ï¸ Weather Data Pipeline Project

A complete **end-to-end Data Engineering pipeline** that automates **weather data collection, transformation, modeling, and visualization** using **Python, DBT, Airflow, PostgreSQL, Docker, and Apache Superset**.

---

## ğŸ§© Project Overview

![Project Overview](docs/images/weather-data.png)

This project automatically:
- Fetches real-time weather data from the **Open-Meteo API**
- Cleans and transforms it using **Pandas**
- Loads it into a **PostgreSQL** database
- Models and aggregates it using **DBT**
- Automates all tasks with **Airflow**
- Visualizes insights through **Apache Superset** dashboards

---

## âš™ï¸ Project Structure

.
â”œâ”€â”€ README.md
â”œâ”€â”€ airflow/
â”‚ â””â”€â”€ dags/
â”œâ”€â”€ api-request/
â”‚ â”œâ”€â”€ api_request.py
â”‚ â”œâ”€â”€ insert_records.py
â”‚ â””â”€â”€ transform_data.py
â”œâ”€â”€ dbt/
â”‚ â”œâ”€â”€ my_project/
â”‚ â”œâ”€â”€ logs/
â”‚ â””â”€â”€ profiles.yml
â”œâ”€â”€ docker/
â”‚ â”œâ”€â”€ docker-bootstrap.sh
â”‚ â”œâ”€â”€ docker-init.sh
â”‚ â””â”€â”€ superset_config.py
â”œâ”€â”€ docker-compose.yml
â”œâ”€â”€ docs/
â”‚ â”œâ”€â”€ dashboard/
â”‚ â””â”€â”€ images/
â”‚ â””â”€â”€ weather-data.png
â”œâ”€â”€ logs/
â”‚ â””â”€â”€ dbt.log
â”œâ”€â”€ postgres/
â”‚ â”œâ”€â”€ airflow_init.sql
â”‚ â””â”€â”€ superset_init.sql
â””â”€â”€ venv/
â”œâ”€â”€ bin/
â”œâ”€â”€ lib/
â””â”€â”€ pyvenv.cfg


## ğŸš€ Quick Start â€” Run the Full Pipeline

### ğŸª„ 1. Clone the Repository
bash
git clone https://github.com/noran-salm/weather-data-project.git
cd weather-data-project
ğŸ³ 2. Start the Environment with Docker
bash
Copy code
docker-compose up --build
This will start:

PostgreSQL

Apache Airflow

DBT container

Apache Superset

ğŸ’¡ Wait 2â€“3 minutes for all services to initialize.

ğŸ§­ 3. Access Services
Service	URL	Default Credentials
Airflow	http://localhost:8080	admin / (password shown in terminal)
Superset	http://localhost:8088	admin / 123456

âš™ï¸ 4. Initialize Superset (First Time Only)
bash
docker exec -it superset_container superset fab create-admin
docker exec -it superset_container superset db upgrade
docker exec -it superset_container superset init
ğŸ§  5. Run the Pipeline
ğŸŸ¢ Option 1 â€” From Airflow
Open Airflow UI â†’ DAGs

Enable weather-api-dbt-orchestrator

Click Trigger DAG

ğŸŸ£ Option 2 â€” Manually
bash
docker exec -it airflow_container python /opt/airflow/api-request/insert_records.py
docker exec -it dbt_container dbt run

ğŸ“Š 6. View Dashboards in Superset

Open http://localhost:8088
â†’ Dashboards â†’ Weather Report
â†’ Enjoy your live weather analytics!

ğŸ’¡ Optimization & Recommendations
Area	Recommendation
API Efficiency	Cache or limit API calls to reduce network cost
Database Performance	Add indexes on city and weather_time_local
Airflow Scheduling	Run the DAG hourly or daily depending on update frequency
DBT Models	Use incremental models instead of full reloads
Monitoring	Add Slack/email alerts for failed Airflow tasks
Superset UX	Add city/date filters for better interactivity

ğŸ–¼ï¸ Adding Images to README
Place your screenshots inside:

bash
docs/images/
Example:

bash
docs/images/weather-data.png
docs/images/dashboard_trend.png
Add them to the README using Markdown:

markdown
Copy code
![Dashboard Overview](docs/images/dashboard_trend.png)
Then commit and push:

bash
Copy code
git add README.md docs/images/*
git commit -m "Add dashboard screenshots"
git push origin main
ğŸ“š References
ğŸŒ¦ Open-Meteo API Documentation

ğŸª¶ Apache Airflow Docs

ğŸ“Š DBT Core Docs

ğŸ“ˆ Apache Superset Docs

ğŸ³ Docker Documentation

ğŸ§¾ License
MIT License Â© 2025 Noran Salm
